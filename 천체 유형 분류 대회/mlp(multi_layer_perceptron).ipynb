{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mlp(multi-layer perceptron).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPC2UPBYLNyO",
        "outputId": "d1e6748d-a464-425d-ef1f-98f263bb914c"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghc-MBTFMn9k",
        "outputId": "2a65c722-23d9-4499-bbf1-5bbd1d98c683"
      },
      "source": [
        "cd drive/MyDrive/dacon/천체 유형 분류 대회"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dacon/천체 유형 분류 대회\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwMt5xJmMdP0"
      },
      "source": [
        "[Keras](https://wikidocs.net/32105)  \n",
        "[Keras로 MLP하기](https://iostream.tistory.com/111)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MeOVcWkLTIz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler # 스케일링 함수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQE2ojMSLUcO"
      },
      "source": [
        "train = pd.read_csv('train.csv', index_col='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "uteU3Sf9LUlB",
        "outputId": "465551e0-054b-42ff-ed17-c4d046da7218"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>fiberID</th>\n",
              "      <th>psfMag_u</th>\n",
              "      <th>psfMag_g</th>\n",
              "      <th>psfMag_r</th>\n",
              "      <th>psfMag_i</th>\n",
              "      <th>psfMag_z</th>\n",
              "      <th>fiberMag_u</th>\n",
              "      <th>fiberMag_g</th>\n",
              "      <th>fiberMag_r</th>\n",
              "      <th>fiberMag_i</th>\n",
              "      <th>fiberMag_z</th>\n",
              "      <th>petroMag_u</th>\n",
              "      <th>petroMag_g</th>\n",
              "      <th>petroMag_r</th>\n",
              "      <th>petroMag_i</th>\n",
              "      <th>petroMag_z</th>\n",
              "      <th>modelMag_u</th>\n",
              "      <th>modelMag_g</th>\n",
              "      <th>modelMag_r</th>\n",
              "      <th>modelMag_i</th>\n",
              "      <th>modelMag_z</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QSO</td>\n",
              "      <td>601</td>\n",
              "      <td>23.198224</td>\n",
              "      <td>21.431953</td>\n",
              "      <td>21.314148</td>\n",
              "      <td>21.176553</td>\n",
              "      <td>21.171444</td>\n",
              "      <td>22.581309</td>\n",
              "      <td>21.644453</td>\n",
              "      <td>21.657571</td>\n",
              "      <td>21.387653</td>\n",
              "      <td>21.572827</td>\n",
              "      <td>22.504317</td>\n",
              "      <td>21.431636</td>\n",
              "      <td>21.478312</td>\n",
              "      <td>21.145409</td>\n",
              "      <td>20.422446</td>\n",
              "      <td>22.749241</td>\n",
              "      <td>21.465534</td>\n",
              "      <td>21.364187</td>\n",
              "      <td>21.020605</td>\n",
              "      <td>21.147340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QSO</td>\n",
              "      <td>788</td>\n",
              "      <td>21.431355</td>\n",
              "      <td>20.708104</td>\n",
              "      <td>20.678850</td>\n",
              "      <td>20.703420</td>\n",
              "      <td>20.473229</td>\n",
              "      <td>21.868797</td>\n",
              "      <td>21.029773</td>\n",
              "      <td>20.967054</td>\n",
              "      <td>20.937731</td>\n",
              "      <td>21.063646</td>\n",
              "      <td>21.360701</td>\n",
              "      <td>20.778968</td>\n",
              "      <td>20.889705</td>\n",
              "      <td>20.639812</td>\n",
              "      <td>20.646660</td>\n",
              "      <td>21.492955</td>\n",
              "      <td>20.758527</td>\n",
              "      <td>20.753925</td>\n",
              "      <td>20.693389</td>\n",
              "      <td>20.512314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>QSO</td>\n",
              "      <td>427</td>\n",
              "      <td>17.851451</td>\n",
              "      <td>16.727898</td>\n",
              "      <td>16.679677</td>\n",
              "      <td>16.694640</td>\n",
              "      <td>16.641788</td>\n",
              "      <td>18.171890</td>\n",
              "      <td>17.033098</td>\n",
              "      <td>16.999682</td>\n",
              "      <td>17.095999</td>\n",
              "      <td>17.076449</td>\n",
              "      <td>17.867253</td>\n",
              "      <td>16.738784</td>\n",
              "      <td>16.688874</td>\n",
              "      <td>16.744210</td>\n",
              "      <td>16.808006</td>\n",
              "      <td>17.818063</td>\n",
              "      <td>16.697434</td>\n",
              "      <td>16.641249</td>\n",
              "      <td>16.660177</td>\n",
              "      <td>16.688928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>QSO</td>\n",
              "      <td>864</td>\n",
              "      <td>20.789900</td>\n",
              "      <td>20.040371</td>\n",
              "      <td>19.926909</td>\n",
              "      <td>19.843840</td>\n",
              "      <td>19.463270</td>\n",
              "      <td>21.039030</td>\n",
              "      <td>20.317165</td>\n",
              "      <td>20.217898</td>\n",
              "      <td>20.073852</td>\n",
              "      <td>19.794505</td>\n",
              "      <td>20.433907</td>\n",
              "      <td>19.993727</td>\n",
              "      <td>19.985531</td>\n",
              "      <td>19.750917</td>\n",
              "      <td>19.455117</td>\n",
              "      <td>20.770711</td>\n",
              "      <td>20.001699</td>\n",
              "      <td>19.889798</td>\n",
              "      <td>19.758113</td>\n",
              "      <td>19.552855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>STAR_RED_DWARF</td>\n",
              "      <td>612</td>\n",
              "      <td>26.454969</td>\n",
              "      <td>23.058767</td>\n",
              "      <td>21.471406</td>\n",
              "      <td>19.504961</td>\n",
              "      <td>18.389096</td>\n",
              "      <td>25.700632</td>\n",
              "      <td>23.629122</td>\n",
              "      <td>21.742750</td>\n",
              "      <td>19.861718</td>\n",
              "      <td>18.810375</td>\n",
              "      <td>25.859229</td>\n",
              "      <td>22.426929</td>\n",
              "      <td>21.673551</td>\n",
              "      <td>19.610012</td>\n",
              "      <td>18.376141</td>\n",
              "      <td>24.877052</td>\n",
              "      <td>23.147993</td>\n",
              "      <td>21.475342</td>\n",
              "      <td>19.487330</td>\n",
              "      <td>18.375655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              type  fiberID   psfMag_u  ...  modelMag_r  modelMag_i  modelMag_z\n",
              "id                                      ...                                    \n",
              "0              QSO      601  23.198224  ...   21.364187   21.020605   21.147340\n",
              "1              QSO      788  21.431355  ...   20.753925   20.693389   20.512314\n",
              "2              QSO      427  17.851451  ...   16.641249   16.660177   16.688928\n",
              "3              QSO      864  20.789900  ...   19.889798   19.758113   19.552855\n",
              "4   STAR_RED_DWARF      612  26.454969  ...   21.475342   19.487330   18.375655\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDS8BitHLUn2"
      },
      "source": [
        "unique_labels = train['type'].unique()  # type 종류 파악\n",
        "label_dict = {val: i for i, val in enumerate(unique_labels)} # 번호를 주고 dict으로\n",
        "i2lb = {v:k for k, v in label_dict.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1ZK3Wl8LUqv"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "labels = train['type']\n",
        "train = train.drop(columns=['fiberID', 'type']) # fiberID는 1000개의 categorical feature이며, 이 커널에서는 무시합니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN_CxaLVLUt2"
      },
      "source": [
        "_mat = scaler.fit_transform(train) # 학습용 데이터\n",
        "train = pd.DataFrame(_mat, columns=train.columns, index=train.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSn0rOgVRFdl"
      },
      "source": [
        "scaler는 fit과 transform 메서드를 지니고 있습니다. fit 메서드로 데이터 변환을 학습하고, transform 메서드로 실제 데이터의 스케일을 조정합니다. \n",
        "\n",
        "이때, fit 메서드는 학습용 데이터에만 적용해야 합니다. 그 후, transform 메서드를 학습용 데이터와 테스트 데이터에 적용합니다. scaler는 fit_transform()이란 단축 메서드를 제공합니다. 학습용 데이터에는 fit_transform()메서드를 적용하고, 테스트 데이터에는 transform()메서드를 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7KRPMdnMCy2"
      },
      "source": [
        "train_x = train\n",
        "train_y = labels.replace(label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "Mj_dDkqoDWzd",
        "outputId": "9d99e95a-c022-4e72-ea18-1de40e7f12ad"
      },
      "source": [
        "train_x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>psfMag_u</th>\n",
              "      <th>psfMag_g</th>\n",
              "      <th>psfMag_r</th>\n",
              "      <th>psfMag_i</th>\n",
              "      <th>psfMag_z</th>\n",
              "      <th>fiberMag_u</th>\n",
              "      <th>fiberMag_g</th>\n",
              "      <th>fiberMag_r</th>\n",
              "      <th>fiberMag_i</th>\n",
              "      <th>fiberMag_z</th>\n",
              "      <th>petroMag_u</th>\n",
              "      <th>petroMag_g</th>\n",
              "      <th>petroMag_r</th>\n",
              "      <th>petroMag_i</th>\n",
              "      <th>petroMag_z</th>\n",
              "      <th>modelMag_u</th>\n",
              "      <th>modelMag_g</th>\n",
              "      <th>modelMag_r</th>\n",
              "      <th>modelMag_i</th>\n",
              "      <th>modelMag_z</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.002522</td>\n",
              "      <td>0.017736</td>\n",
              "      <td>0.022913</td>\n",
              "      <td>0.026865</td>\n",
              "      <td>0.028350</td>\n",
              "      <td>0.002812</td>\n",
              "      <td>0.003432</td>\n",
              "      <td>0.028019</td>\n",
              "      <td>0.026184</td>\n",
              "      <td>0.024489</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.019287</td>\n",
              "      <td>0.030818</td>\n",
              "      <td>0.023734</td>\n",
              "      <td>0.019085</td>\n",
              "      <td>0.021572</td>\n",
              "      <td>0.018062</td>\n",
              "      <td>0.023754</td>\n",
              "      <td>0.025371</td>\n",
              "      <td>0.029607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.002373</td>\n",
              "      <td>0.013079</td>\n",
              "      <td>0.017916</td>\n",
              "      <td>0.022808</td>\n",
              "      <td>0.022707</td>\n",
              "      <td>0.002641</td>\n",
              "      <td>0.002612</td>\n",
              "      <td>0.020351</td>\n",
              "      <td>0.022507</td>\n",
              "      <td>0.020998</td>\n",
              "      <td>-0.000604</td>\n",
              "      <td>0.015060</td>\n",
              "      <td>0.024765</td>\n",
              "      <td>0.020265</td>\n",
              "      <td>0.020656</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.013691</td>\n",
              "      <td>0.019199</td>\n",
              "      <td>0.022876</td>\n",
              "      <td>0.024857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002071</td>\n",
              "      <td>-0.012530</td>\n",
              "      <td>-0.013542</td>\n",
              "      <td>-0.011566</td>\n",
              "      <td>-0.008257</td>\n",
              "      <td>0.001755</td>\n",
              "      <td>-0.002722</td>\n",
              "      <td>-0.023707</td>\n",
              "      <td>-0.008885</td>\n",
              "      <td>-0.006338</td>\n",
              "      <td>-0.005030</td>\n",
              "      <td>-0.011112</td>\n",
              "      <td>-0.018435</td>\n",
              "      <td>-0.006467</td>\n",
              "      <td>-0.006246</td>\n",
              "      <td>-0.018749</td>\n",
              "      <td>-0.011420</td>\n",
              "      <td>-0.011496</td>\n",
              "      <td>-0.007869</td>\n",
              "      <td>-0.003743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.002319</td>\n",
              "      <td>0.008782</td>\n",
              "      <td>0.012001</td>\n",
              "      <td>0.015437</td>\n",
              "      <td>0.014545</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.001661</td>\n",
              "      <td>0.012031</td>\n",
              "      <td>0.015448</td>\n",
              "      <td>0.012297</td>\n",
              "      <td>-0.001778</td>\n",
              "      <td>0.009973</td>\n",
              "      <td>0.015467</td>\n",
              "      <td>0.014165</td>\n",
              "      <td>0.012306</td>\n",
              "      <td>0.005394</td>\n",
              "      <td>0.009011</td>\n",
              "      <td>0.012750</td>\n",
              "      <td>0.015747</td>\n",
              "      <td>0.017680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.002796</td>\n",
              "      <td>0.028203</td>\n",
              "      <td>0.024150</td>\n",
              "      <td>0.012532</td>\n",
              "      <td>0.005864</td>\n",
              "      <td>0.003560</td>\n",
              "      <td>0.006081</td>\n",
              "      <td>0.028965</td>\n",
              "      <td>0.013715</td>\n",
              "      <td>0.005550</td>\n",
              "      <td>0.005094</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>0.032826</td>\n",
              "      <td>0.013198</td>\n",
              "      <td>0.004744</td>\n",
              "      <td>0.038971</td>\n",
              "      <td>0.028465</td>\n",
              "      <td>0.024583</td>\n",
              "      <td>0.013683</td>\n",
              "      <td>0.008874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199986</th>\n",
              "      <td>0.002305</td>\n",
              "      <td>0.011340</td>\n",
              "      <td>0.012975</td>\n",
              "      <td>0.016956</td>\n",
              "      <td>0.018557</td>\n",
              "      <td>0.002421</td>\n",
              "      <td>0.002208</td>\n",
              "      <td>0.013072</td>\n",
              "      <td>0.017487</td>\n",
              "      <td>0.015154</td>\n",
              "      <td>-0.001269</td>\n",
              "      <td>0.013488</td>\n",
              "      <td>0.016917</td>\n",
              "      <td>0.016580</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>0.005497</td>\n",
              "      <td>0.011022</td>\n",
              "      <td>0.013677</td>\n",
              "      <td>0.018063</td>\n",
              "      <td>0.019997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199987</th>\n",
              "      <td>0.002697</td>\n",
              "      <td>0.026585</td>\n",
              "      <td>0.020991</td>\n",
              "      <td>0.018385</td>\n",
              "      <td>0.015615</td>\n",
              "      <td>0.003794</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.018342</td>\n",
              "      <td>0.014533</td>\n",
              "      <td>0.009983</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>0.024592</td>\n",
              "      <td>0.016559</td>\n",
              "      <td>0.008915</td>\n",
              "      <td>0.007738</td>\n",
              "      <td>0.055691</td>\n",
              "      <td>0.021073</td>\n",
              "      <td>0.011903</td>\n",
              "      <td>0.009892</td>\n",
              "      <td>0.010016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199988</th>\n",
              "      <td>0.002556</td>\n",
              "      <td>0.020765</td>\n",
              "      <td>0.015178</td>\n",
              "      <td>0.003550</td>\n",
              "      <td>-0.002722</td>\n",
              "      <td>0.003229</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>0.017516</td>\n",
              "      <td>0.005125</td>\n",
              "      <td>-0.001912</td>\n",
              "      <td>0.005250</td>\n",
              "      <td>0.023831</td>\n",
              "      <td>0.019006</td>\n",
              "      <td>0.005632</td>\n",
              "      <td>-0.001967</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.020734</td>\n",
              "      <td>0.016711</td>\n",
              "      <td>0.005723</td>\n",
              "      <td>0.001066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199989</th>\n",
              "      <td>0.002351</td>\n",
              "      <td>0.012134</td>\n",
              "      <td>0.012826</td>\n",
              "      <td>0.015007</td>\n",
              "      <td>0.015169</td>\n",
              "      <td>0.002602</td>\n",
              "      <td>0.001865</td>\n",
              "      <td>0.008191</td>\n",
              "      <td>0.011607</td>\n",
              "      <td>0.009406</td>\n",
              "      <td>-0.003746</td>\n",
              "      <td>-0.002517</td>\n",
              "      <td>-0.010080</td>\n",
              "      <td>-0.003472</td>\n",
              "      <td>-0.003672</td>\n",
              "      <td>-0.008192</td>\n",
              "      <td>-0.003335</td>\n",
              "      <td>-0.005532</td>\n",
              "      <td>-0.003801</td>\n",
              "      <td>-0.001641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199990</th>\n",
              "      <td>0.002305</td>\n",
              "      <td>0.009309</td>\n",
              "      <td>0.015189</td>\n",
              "      <td>0.019868</td>\n",
              "      <td>0.018503</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.017149</td>\n",
              "      <td>0.020083</td>\n",
              "      <td>0.016923</td>\n",
              "      <td>-0.001319</td>\n",
              "      <td>0.010854</td>\n",
              "      <td>0.019888</td>\n",
              "      <td>0.019710</td>\n",
              "      <td>0.014836</td>\n",
              "      <td>0.005209</td>\n",
              "      <td>0.009720</td>\n",
              "      <td>0.016016</td>\n",
              "      <td>0.019246</td>\n",
              "      <td>0.020988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>199991 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        psfMag_u  psfMag_g  psfMag_r  ...  modelMag_r  modelMag_i  modelMag_z\n",
              "id                                    ...                                    \n",
              "0       0.002522  0.017736  0.022913  ...    0.023754    0.025371    0.029607\n",
              "1       0.002373  0.013079  0.017916  ...    0.019199    0.022876    0.024857\n",
              "2       0.002071 -0.012530 -0.013542  ...   -0.011496   -0.007869   -0.003743\n",
              "3       0.002319  0.008782  0.012001  ...    0.012750    0.015747    0.017680\n",
              "4       0.002796  0.028203  0.024150  ...    0.024583    0.013683    0.008874\n",
              "...          ...       ...       ...  ...         ...         ...         ...\n",
              "199986  0.002305  0.011340  0.012975  ...    0.013677    0.018063    0.019997\n",
              "199987  0.002697  0.026585  0.020991  ...    0.011903    0.009892    0.010016\n",
              "199988  0.002556  0.020765  0.015178  ...    0.016711    0.005723    0.001066\n",
              "199989  0.002351  0.012134  0.012826  ...   -0.005532   -0.003801   -0.001641\n",
              "199990  0.002305  0.009309  0.015189  ...    0.016016    0.019246    0.020988\n",
              "\n",
              "[199991 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YcVqDoEDY5O",
        "outputId": "253a2f50-74ff-4b28-9602-ff677daeb067"
      },
      "source": [
        "train_y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0         0\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         1\n",
              "         ..\n",
              "199986    0\n",
              "199987    6\n",
              "199988    1\n",
              "199989    6\n",
              "199990    0\n",
              "Name: type, Length: 199991, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHZ--ggBMEVI"
      },
      "source": [
        "# 학습률 설정? 0.0003으로 지정\n",
        "# Adam 알고리즘을 사용 / 왜 Adam을 썼나?\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0003,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vZNNT4Fg3LN"
      },
      "source": [
        "[학습률 설정](https://forensics.tistory.com/28)  \n",
        "[옵티마이저](https://hiddenbeginner.github.io/deeplearning/2019/09/22/optimization_algorithms_in_deep_learning.html)  \n",
        "[경사하강법](https://seamless.tistory.com/38)  \n",
        "  \n",
        "옵티마이저 (Optimizer)는 손실 함수을 통해 얻은 손실값으로부터 모델을 업데이트하는 방식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssfvw5SvMHN-"
      },
      "source": [
        "model = tf.keras.models.Sequential([   # 가장 간단한 신경망 모델\n",
        "  tf.keras.layers.Input(len(train_x.columns)),\n",
        "  tf.keras.layers.Dense(256*4, activation='elu'), # 계층 / 신경망을 만드는 것? / units 갯수\n",
        "  tf.keras.layers.Dropout(0.1), # 정규화 dropout / 인풋의 10%를 drop\n",
        "  tf.keras.layers.Dense(256*4, activation='elu'),\n",
        "  tf.keras.layers.Dense(256*3, activation='elu'),\n",
        "  tf.keras.layers.Dropout(0.15),\n",
        "  tf.keras.layers.Dense(256*3, activation='elu'),\n",
        "  tf.keras.layers.Dense(256*2, activation='elu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(256*2, activation='elu'),\n",
        "  tf.keras.layers.Dropout(0.15),\n",
        "  tf.keras.layers.Dense(256*1, activation='elu'),\n",
        "  tf.keras.layers.Dense(256*1, activation='elu'),\n",
        "  tf.keras.layers.Dropout(0.15),\n",
        "  tf.keras.layers.Dense(128, activation='elu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(64, activation='elu'),\n",
        "  tf.keras.layers.Dense(32, activation='elu'),\n",
        "  tf.keras.layers.Dense(19, activation='softmax') # 출력층 / 확률값 출력을 위해 softmax / sigmoid는 이진분류 / input이 20개이므로 출력층은 19\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPF0eG7QmOab"
      },
      "source": [
        "[Sequential Model](https://www.tensorflow.org/guide/keras/sequential_model?hl=ko)  \n",
        "[Keras Dense](https://han-py.tistory.com/207)  \n",
        "[activation function_1](https://m.blog.naver.com/wideeyed/221017173808)  \n",
        "[activation function_2](https://seongkyun.github.io/study/2019/05/01/activations/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbbldI6cMJRn"
      },
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',  # 손실함수, 최소화 시켜야하는 것 / 다중 클래스 분류\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXkahSnFMLS2",
        "outputId": "939e3caf-fb32-4554-ab58-7ed9bf7bb937"
      },
      "source": [
        "model.fit(train_x,\n",
        "          train_y,\n",
        "          batch_size=256*3, # 학습시 데이터 개수\n",
        "          validation_split=0.1,  # 훈련데이터 10%를 검증데이터로 사용\n",
        "          epochs=200)  # 훈련횟수가 200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "235/235 [==============================] - 3s 9ms/step - loss: 1.4669 - accuracy: 0.5753 - val_loss: 0.6865 - val_accuracy: 0.7684\n",
            "Epoch 2/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.7080 - accuracy: 0.7627 - val_loss: 0.5750 - val_accuracy: 0.8025\n",
            "Epoch 3/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.6349 - accuracy: 0.7817 - val_loss: 0.5321 - val_accuracy: 0.8220\n",
            "Epoch 4/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.5917 - accuracy: 0.7984 - val_loss: 0.5398 - val_accuracy: 0.8104\n",
            "Epoch 5/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.5616 - accuracy: 0.8089 - val_loss: 0.5146 - val_accuracy: 0.8235\n",
            "Epoch 6/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.5340 - accuracy: 0.8173 - val_loss: 0.5109 - val_accuracy: 0.8263\n",
            "Epoch 7/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.5422 - accuracy: 0.8157 - val_loss: 0.4715 - val_accuracy: 0.8377\n",
            "Epoch 8/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.5180 - accuracy: 0.8238 - val_loss: 0.4664 - val_accuracy: 0.8390\n",
            "Epoch 9/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.5080 - accuracy: 0.8260 - val_loss: 0.4509 - val_accuracy: 0.8428\n",
            "Epoch 10/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.5043 - accuracy: 0.8260 - val_loss: 0.4797 - val_accuracy: 0.8346\n",
            "Epoch 11/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4954 - accuracy: 0.8293 - val_loss: 0.4483 - val_accuracy: 0.8427\n",
            "Epoch 12/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4875 - accuracy: 0.8328 - val_loss: 0.4422 - val_accuracy: 0.8462\n",
            "Epoch 13/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4788 - accuracy: 0.8353 - val_loss: 0.4909 - val_accuracy: 0.8322\n",
            "Epoch 14/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4858 - accuracy: 0.8329 - val_loss: 0.4533 - val_accuracy: 0.8434\n",
            "Epoch 15/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4761 - accuracy: 0.8349 - val_loss: 0.4574 - val_accuracy: 0.8406\n",
            "Epoch 16/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4754 - accuracy: 0.8366 - val_loss: 0.4373 - val_accuracy: 0.8461\n",
            "Epoch 17/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4701 - accuracy: 0.8383 - val_loss: 0.4442 - val_accuracy: 0.8424\n",
            "Epoch 18/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4727 - accuracy: 0.8371 - val_loss: 0.4369 - val_accuracy: 0.8449\n",
            "Epoch 19/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4642 - accuracy: 0.8400 - val_loss: 0.4227 - val_accuracy: 0.8504\n",
            "Epoch 20/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4560 - accuracy: 0.8417 - val_loss: 0.4321 - val_accuracy: 0.8493\n",
            "Epoch 21/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4584 - accuracy: 0.8417 - val_loss: 0.4395 - val_accuracy: 0.8468\n",
            "Epoch 22/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4604 - accuracy: 0.8402 - val_loss: 0.4326 - val_accuracy: 0.8477\n",
            "Epoch 23/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4568 - accuracy: 0.8398 - val_loss: 0.4330 - val_accuracy: 0.8526\n",
            "Epoch 24/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4524 - accuracy: 0.8429 - val_loss: 0.4229 - val_accuracy: 0.8534\n",
            "Epoch 25/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4562 - accuracy: 0.8406 - val_loss: 0.4376 - val_accuracy: 0.8485\n",
            "Epoch 26/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4553 - accuracy: 0.8404 - val_loss: 0.4151 - val_accuracy: 0.8552\n",
            "Epoch 27/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4500 - accuracy: 0.8424 - val_loss: 0.4401 - val_accuracy: 0.8419\n",
            "Epoch 28/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4475 - accuracy: 0.8446 - val_loss: 0.4181 - val_accuracy: 0.8539\n",
            "Epoch 29/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4454 - accuracy: 0.8451 - val_loss: 0.4152 - val_accuracy: 0.8558\n",
            "Epoch 30/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4433 - accuracy: 0.8457 - val_loss: 0.4275 - val_accuracy: 0.8485\n",
            "Epoch 31/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4523 - accuracy: 0.8411 - val_loss: 0.4221 - val_accuracy: 0.8510\n",
            "Epoch 32/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4448 - accuracy: 0.8452 - val_loss: 0.4232 - val_accuracy: 0.8504\n",
            "Epoch 33/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4402 - accuracy: 0.8468 - val_loss: 0.4114 - val_accuracy: 0.8550\n",
            "Epoch 34/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4385 - accuracy: 0.8457 - val_loss: 0.4101 - val_accuracy: 0.8572\n",
            "Epoch 35/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4367 - accuracy: 0.8484 - val_loss: 0.4196 - val_accuracy: 0.8497\n",
            "Epoch 36/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4430 - accuracy: 0.8454 - val_loss: 0.4096 - val_accuracy: 0.8558\n",
            "Epoch 37/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4303 - accuracy: 0.8497 - val_loss: 0.4055 - val_accuracy: 0.8572\n",
            "Epoch 38/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4443 - accuracy: 0.8458 - val_loss: 0.4087 - val_accuracy: 0.8581\n",
            "Epoch 39/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4271 - accuracy: 0.8515 - val_loss: 0.4277 - val_accuracy: 0.8510\n",
            "Epoch 40/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4309 - accuracy: 0.8496 - val_loss: 0.4152 - val_accuracy: 0.8534\n",
            "Epoch 41/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4354 - accuracy: 0.8478 - val_loss: 0.4094 - val_accuracy: 0.8559\n",
            "Epoch 42/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4261 - accuracy: 0.8505 - val_loss: 0.4432 - val_accuracy: 0.8501\n",
            "Epoch 43/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4376 - accuracy: 0.8488 - val_loss: 0.4887 - val_accuracy: 0.8221\n",
            "Epoch 44/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4288 - accuracy: 0.8501 - val_loss: 0.3995 - val_accuracy: 0.8598\n",
            "Epoch 45/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4241 - accuracy: 0.8516 - val_loss: 0.4150 - val_accuracy: 0.8537\n",
            "Epoch 46/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4224 - accuracy: 0.8523 - val_loss: 0.4041 - val_accuracy: 0.8578\n",
            "Epoch 47/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4173 - accuracy: 0.8547 - val_loss: 0.4068 - val_accuracy: 0.8550\n",
            "Epoch 48/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4227 - accuracy: 0.8506 - val_loss: 0.4011 - val_accuracy: 0.8599\n",
            "Epoch 49/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4206 - accuracy: 0.8524 - val_loss: 0.4046 - val_accuracy: 0.8576\n",
            "Epoch 50/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4209 - accuracy: 0.8516 - val_loss: 0.4081 - val_accuracy: 0.8568\n",
            "Epoch 51/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4204 - accuracy: 0.8526 - val_loss: 0.4172 - val_accuracy: 0.8530\n",
            "Epoch 52/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4152 - accuracy: 0.8541 - val_loss: 0.3975 - val_accuracy: 0.8607\n",
            "Epoch 53/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4174 - accuracy: 0.8531 - val_loss: 0.3995 - val_accuracy: 0.8595\n",
            "Epoch 54/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4119 - accuracy: 0.8546 - val_loss: 0.4006 - val_accuracy: 0.8573\n",
            "Epoch 55/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4138 - accuracy: 0.8544 - val_loss: 0.4084 - val_accuracy: 0.8569\n",
            "Epoch 56/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4139 - accuracy: 0.8552 - val_loss: 0.4031 - val_accuracy: 0.8578\n",
            "Epoch 57/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4142 - accuracy: 0.8545 - val_loss: 0.3890 - val_accuracy: 0.8644\n",
            "Epoch 58/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4129 - accuracy: 0.8552 - val_loss: 0.4127 - val_accuracy: 0.8573\n",
            "Epoch 59/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4150 - accuracy: 0.8535 - val_loss: 0.4107 - val_accuracy: 0.8558\n",
            "Epoch 60/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4117 - accuracy: 0.8551 - val_loss: 0.4119 - val_accuracy: 0.8548\n",
            "Epoch 61/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4114 - accuracy: 0.8557 - val_loss: 0.4031 - val_accuracy: 0.8613\n",
            "Epoch 62/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4104 - accuracy: 0.8553 - val_loss: 0.4068 - val_accuracy: 0.8572\n",
            "Epoch 63/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4078 - accuracy: 0.8568 - val_loss: 0.3903 - val_accuracy: 0.8633\n",
            "Epoch 64/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4070 - accuracy: 0.8569 - val_loss: 0.3898 - val_accuracy: 0.8626\n",
            "Epoch 65/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4079 - accuracy: 0.8562 - val_loss: 0.4120 - val_accuracy: 0.8535\n",
            "Epoch 66/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4115 - accuracy: 0.8552 - val_loss: 0.3887 - val_accuracy: 0.8647\n",
            "Epoch 67/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4068 - accuracy: 0.8564 - val_loss: 0.3971 - val_accuracy: 0.8607\n",
            "Epoch 68/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4128 - accuracy: 0.8546 - val_loss: 0.3938 - val_accuracy: 0.8619\n",
            "Epoch 69/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4041 - accuracy: 0.8570 - val_loss: 0.3923 - val_accuracy: 0.8625\n",
            "Epoch 70/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4033 - accuracy: 0.8573 - val_loss: 0.3873 - val_accuracy: 0.8644\n",
            "Epoch 71/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4043 - accuracy: 0.8589 - val_loss: 0.3960 - val_accuracy: 0.8623\n",
            "Epoch 72/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4077 - accuracy: 0.8558 - val_loss: 0.3873 - val_accuracy: 0.8646\n",
            "Epoch 73/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4075 - accuracy: 0.8559 - val_loss: 0.3933 - val_accuracy: 0.8608\n",
            "Epoch 74/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4044 - accuracy: 0.8583 - val_loss: 0.4055 - val_accuracy: 0.8597\n",
            "Epoch 75/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4073 - accuracy: 0.8568 - val_loss: 0.3936 - val_accuracy: 0.8613\n",
            "Epoch 76/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4065 - accuracy: 0.8570 - val_loss: 0.3970 - val_accuracy: 0.8596\n",
            "Epoch 77/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4068 - accuracy: 0.8565 - val_loss: 0.3863 - val_accuracy: 0.8630\n",
            "Epoch 78/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4009 - accuracy: 0.8569 - val_loss: 0.3969 - val_accuracy: 0.8600\n",
            "Epoch 79/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4042 - accuracy: 0.8577 - val_loss: 0.4045 - val_accuracy: 0.8573\n",
            "Epoch 80/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3982 - accuracy: 0.8596 - val_loss: 0.3856 - val_accuracy: 0.8644\n",
            "Epoch 81/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4034 - accuracy: 0.8586 - val_loss: 0.4082 - val_accuracy: 0.8554\n",
            "Epoch 82/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4111 - accuracy: 0.8551 - val_loss: 0.4175 - val_accuracy: 0.8528\n",
            "Epoch 83/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4059 - accuracy: 0.8557 - val_loss: 0.4086 - val_accuracy: 0.8570\n",
            "Epoch 84/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.4028 - accuracy: 0.8576 - val_loss: 0.3973 - val_accuracy: 0.8636\n",
            "Epoch 85/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3987 - accuracy: 0.8589 - val_loss: 0.3866 - val_accuracy: 0.8623\n",
            "Epoch 86/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3998 - accuracy: 0.8575 - val_loss: 0.4131 - val_accuracy: 0.8547\n",
            "Epoch 87/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3993 - accuracy: 0.8593 - val_loss: 0.3947 - val_accuracy: 0.8610\n",
            "Epoch 88/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4011 - accuracy: 0.8569 - val_loss: 0.3900 - val_accuracy: 0.8628\n",
            "Epoch 89/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3949 - accuracy: 0.8605 - val_loss: 0.3894 - val_accuracy: 0.8637\n",
            "Epoch 90/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3990 - accuracy: 0.8596 - val_loss: 0.3865 - val_accuracy: 0.8647\n",
            "Epoch 91/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.4001 - accuracy: 0.8587 - val_loss: 0.3891 - val_accuracy: 0.8639\n",
            "Epoch 92/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3957 - accuracy: 0.8598 - val_loss: 0.3908 - val_accuracy: 0.8632\n",
            "Epoch 93/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3931 - accuracy: 0.8615 - val_loss: 0.4008 - val_accuracy: 0.8594\n",
            "Epoch 94/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3994 - accuracy: 0.8585 - val_loss: 0.3939 - val_accuracy: 0.8586\n",
            "Epoch 95/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3906 - accuracy: 0.8613 - val_loss: 0.3895 - val_accuracy: 0.8647\n",
            "Epoch 96/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3980 - accuracy: 0.8594 - val_loss: 0.3863 - val_accuracy: 0.8633\n",
            "Epoch 97/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3929 - accuracy: 0.8604 - val_loss: 0.3938 - val_accuracy: 0.8619\n",
            "Epoch 98/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3899 - accuracy: 0.8616 - val_loss: 0.3891 - val_accuracy: 0.8634\n",
            "Epoch 99/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3928 - accuracy: 0.8613 - val_loss: 0.3944 - val_accuracy: 0.8618\n",
            "Epoch 100/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3964 - accuracy: 0.8599 - val_loss: 0.3973 - val_accuracy: 0.8647\n",
            "Epoch 101/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3947 - accuracy: 0.8599 - val_loss: 0.4027 - val_accuracy: 0.8590\n",
            "Epoch 102/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3959 - accuracy: 0.8597 - val_loss: 0.3825 - val_accuracy: 0.8666\n",
            "Epoch 103/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3868 - accuracy: 0.8635 - val_loss: 0.3814 - val_accuracy: 0.8654\n",
            "Epoch 104/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3846 - accuracy: 0.8632 - val_loss: 0.3879 - val_accuracy: 0.8655\n",
            "Epoch 105/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3863 - accuracy: 0.8630 - val_loss: 0.3854 - val_accuracy: 0.8646\n",
            "Epoch 106/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3980 - accuracy: 0.8597 - val_loss: 0.3835 - val_accuracy: 0.8655\n",
            "Epoch 107/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3830 - accuracy: 0.8640 - val_loss: 0.3809 - val_accuracy: 0.8673\n",
            "Epoch 108/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3830 - accuracy: 0.8630 - val_loss: 0.3992 - val_accuracy: 0.8601\n",
            "Epoch 109/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3870 - accuracy: 0.8626 - val_loss: 0.3857 - val_accuracy: 0.8634\n",
            "Epoch 110/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3836 - accuracy: 0.8631 - val_loss: 0.3797 - val_accuracy: 0.8677\n",
            "Epoch 111/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3860 - accuracy: 0.8629 - val_loss: 0.3789 - val_accuracy: 0.8654\n",
            "Epoch 112/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3827 - accuracy: 0.8638 - val_loss: 0.3883 - val_accuracy: 0.8633\n",
            "Epoch 113/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3876 - accuracy: 0.8623 - val_loss: 0.3812 - val_accuracy: 0.8648\n",
            "Epoch 114/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3862 - accuracy: 0.8633 - val_loss: 0.3902 - val_accuracy: 0.8619\n",
            "Epoch 115/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3863 - accuracy: 0.8620 - val_loss: 0.3803 - val_accuracy: 0.8676\n",
            "Epoch 116/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3837 - accuracy: 0.8630 - val_loss: 0.3760 - val_accuracy: 0.8672\n",
            "Epoch 117/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3802 - accuracy: 0.8648 - val_loss: 0.3805 - val_accuracy: 0.8661\n",
            "Epoch 118/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3851 - accuracy: 0.8623 - val_loss: 0.3853 - val_accuracy: 0.8649\n",
            "Epoch 119/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3808 - accuracy: 0.8647 - val_loss: 0.3787 - val_accuracy: 0.8676\n",
            "Epoch 120/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3824 - accuracy: 0.8636 - val_loss: 0.3808 - val_accuracy: 0.8645\n",
            "Epoch 121/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3856 - accuracy: 0.8623 - val_loss: 0.3742 - val_accuracy: 0.8671\n",
            "Epoch 122/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3774 - accuracy: 0.8652 - val_loss: 0.3783 - val_accuracy: 0.8678\n",
            "Epoch 123/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3800 - accuracy: 0.8650 - val_loss: 0.3752 - val_accuracy: 0.8681\n",
            "Epoch 124/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3815 - accuracy: 0.8629 - val_loss: 0.3877 - val_accuracy: 0.8652\n",
            "Epoch 125/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3820 - accuracy: 0.8643 - val_loss: 0.3810 - val_accuracy: 0.8672\n",
            "Epoch 126/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3822 - accuracy: 0.8646 - val_loss: 0.3731 - val_accuracy: 0.8691\n",
            "Epoch 127/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3799 - accuracy: 0.8649 - val_loss: 0.3798 - val_accuracy: 0.8651\n",
            "Epoch 128/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3793 - accuracy: 0.8643 - val_loss: 0.3744 - val_accuracy: 0.8676\n",
            "Epoch 129/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3759 - accuracy: 0.8662 - val_loss: 0.3749 - val_accuracy: 0.8665\n",
            "Epoch 130/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3777 - accuracy: 0.8651 - val_loss: 0.3763 - val_accuracy: 0.8701\n",
            "Epoch 131/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3821 - accuracy: 0.8632 - val_loss: 0.3834 - val_accuracy: 0.8655\n",
            "Epoch 132/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3767 - accuracy: 0.8652 - val_loss: 0.3751 - val_accuracy: 0.8681\n",
            "Epoch 133/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3742 - accuracy: 0.8660 - val_loss: 0.3781 - val_accuracy: 0.8636\n",
            "Epoch 134/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3757 - accuracy: 0.8663 - val_loss: 0.3752 - val_accuracy: 0.8668\n",
            "Epoch 135/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3763 - accuracy: 0.8657 - val_loss: 0.3792 - val_accuracy: 0.8642\n",
            "Epoch 136/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3760 - accuracy: 0.8653 - val_loss: 0.3720 - val_accuracy: 0.8673\n",
            "Epoch 137/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3762 - accuracy: 0.8661 - val_loss: 0.3869 - val_accuracy: 0.8630\n",
            "Epoch 138/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3756 - accuracy: 0.8647 - val_loss: 0.3772 - val_accuracy: 0.8666\n",
            "Epoch 139/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3723 - accuracy: 0.8665 - val_loss: 0.3777 - val_accuracy: 0.8673\n",
            "Epoch 140/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3705 - accuracy: 0.8673 - val_loss: 0.3687 - val_accuracy: 0.8723\n",
            "Epoch 141/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3724 - accuracy: 0.8672 - val_loss: 0.3771 - val_accuracy: 0.8637\n",
            "Epoch 142/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3697 - accuracy: 0.8678 - val_loss: 0.3759 - val_accuracy: 0.8673\n",
            "Epoch 143/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3775 - accuracy: 0.8656 - val_loss: 0.4006 - val_accuracy: 0.8589\n",
            "Epoch 144/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3775 - accuracy: 0.8652 - val_loss: 0.3774 - val_accuracy: 0.8673\n",
            "Epoch 145/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3709 - accuracy: 0.8676 - val_loss: 0.3751 - val_accuracy: 0.8687\n",
            "Epoch 146/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3755 - accuracy: 0.8655 - val_loss: 0.3802 - val_accuracy: 0.8652\n",
            "Epoch 147/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3726 - accuracy: 0.8663 - val_loss: 0.3775 - val_accuracy: 0.8660\n",
            "Epoch 148/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3669 - accuracy: 0.8690 - val_loss: 0.3706 - val_accuracy: 0.8690\n",
            "Epoch 149/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3689 - accuracy: 0.8681 - val_loss: 0.3708 - val_accuracy: 0.8705\n",
            "Epoch 150/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3654 - accuracy: 0.8695 - val_loss: 0.3732 - val_accuracy: 0.8702\n",
            "Epoch 151/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3706 - accuracy: 0.8678 - val_loss: 0.3828 - val_accuracy: 0.8665\n",
            "Epoch 152/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3672 - accuracy: 0.8685 - val_loss: 0.3720 - val_accuracy: 0.8675\n",
            "Epoch 153/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3684 - accuracy: 0.8676 - val_loss: 0.3670 - val_accuracy: 0.8691\n",
            "Epoch 154/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3723 - accuracy: 0.8676 - val_loss: 0.3758 - val_accuracy: 0.8659\n",
            "Epoch 155/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3703 - accuracy: 0.8686 - val_loss: 0.3727 - val_accuracy: 0.8679\n",
            "Epoch 156/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3669 - accuracy: 0.8684 - val_loss: 0.3794 - val_accuracy: 0.8661\n",
            "Epoch 157/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3643 - accuracy: 0.8703 - val_loss: 0.3847 - val_accuracy: 0.8630\n",
            "Epoch 158/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3744 - accuracy: 0.8651 - val_loss: 0.3728 - val_accuracy: 0.8677\n",
            "Epoch 159/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3691 - accuracy: 0.8670 - val_loss: 0.3751 - val_accuracy: 0.8673\n",
            "Epoch 160/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3625 - accuracy: 0.8698 - val_loss: 0.3666 - val_accuracy: 0.8708\n",
            "Epoch 161/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3655 - accuracy: 0.8689 - val_loss: 0.3674 - val_accuracy: 0.8697\n",
            "Epoch 162/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3655 - accuracy: 0.8684 - val_loss: 0.3827 - val_accuracy: 0.8662\n",
            "Epoch 163/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3684 - accuracy: 0.8676 - val_loss: 0.3777 - val_accuracy: 0.8650\n",
            "Epoch 164/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3641 - accuracy: 0.8690 - val_loss: 0.3746 - val_accuracy: 0.8684\n",
            "Epoch 165/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3646 - accuracy: 0.8695 - val_loss: 0.3702 - val_accuracy: 0.8726\n",
            "Epoch 166/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3670 - accuracy: 0.8686 - val_loss: 0.3974 - val_accuracy: 0.8617\n",
            "Epoch 167/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3705 - accuracy: 0.8675 - val_loss: 0.3737 - val_accuracy: 0.8692\n",
            "Epoch 168/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3576 - accuracy: 0.8719 - val_loss: 0.3673 - val_accuracy: 0.8707\n",
            "Epoch 169/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3621 - accuracy: 0.8699 - val_loss: 0.3666 - val_accuracy: 0.8708\n",
            "Epoch 170/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3652 - accuracy: 0.8689 - val_loss: 0.3741 - val_accuracy: 0.8671\n",
            "Epoch 171/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3621 - accuracy: 0.8705 - val_loss: 0.3755 - val_accuracy: 0.8687\n",
            "Epoch 172/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3629 - accuracy: 0.8694 - val_loss: 0.3745 - val_accuracy: 0.8706\n",
            "Epoch 173/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3597 - accuracy: 0.8719 - val_loss: 0.3754 - val_accuracy: 0.8683\n",
            "Epoch 174/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3698 - accuracy: 0.8675 - val_loss: 0.3700 - val_accuracy: 0.8704\n",
            "Epoch 175/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3685 - accuracy: 0.8672 - val_loss: 0.3744 - val_accuracy: 0.8680\n",
            "Epoch 176/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3641 - accuracy: 0.8684 - val_loss: 0.3738 - val_accuracy: 0.8687\n",
            "Epoch 177/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3655 - accuracy: 0.8687 - val_loss: 0.3672 - val_accuracy: 0.8698\n",
            "Epoch 178/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3607 - accuracy: 0.8704 - val_loss: 0.3839 - val_accuracy: 0.8683\n",
            "Epoch 179/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3627 - accuracy: 0.8684 - val_loss: 0.3728 - val_accuracy: 0.8698\n",
            "Epoch 180/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3574 - accuracy: 0.8706 - val_loss: 0.3693 - val_accuracy: 0.8716\n",
            "Epoch 181/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3575 - accuracy: 0.8713 - val_loss: 0.3759 - val_accuracy: 0.8666\n",
            "Epoch 182/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3694 - accuracy: 0.8685 - val_loss: 0.3752 - val_accuracy: 0.8690\n",
            "Epoch 183/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3591 - accuracy: 0.8710 - val_loss: 0.3775 - val_accuracy: 0.8661\n",
            "Epoch 184/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3617 - accuracy: 0.8706 - val_loss: 0.3713 - val_accuracy: 0.8723\n",
            "Epoch 185/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3603 - accuracy: 0.8698 - val_loss: 0.3820 - val_accuracy: 0.8644\n",
            "Epoch 186/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3580 - accuracy: 0.8710 - val_loss: 0.3714 - val_accuracy: 0.8688\n",
            "Epoch 187/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3565 - accuracy: 0.8719 - val_loss: 0.3732 - val_accuracy: 0.8702\n",
            "Epoch 188/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3571 - accuracy: 0.8720 - val_loss: 0.3880 - val_accuracy: 0.8634\n",
            "Epoch 189/200\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.3581 - accuracy: 0.8707 - val_loss: 0.3715 - val_accuracy: 0.8716\n",
            "Epoch 190/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3560 - accuracy: 0.8714 - val_loss: 0.3670 - val_accuracy: 0.8709\n",
            "Epoch 191/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3526 - accuracy: 0.8733 - val_loss: 0.3777 - val_accuracy: 0.8648\n",
            "Epoch 192/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3595 - accuracy: 0.8699 - val_loss: 0.3789 - val_accuracy: 0.8688\n",
            "Epoch 193/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3551 - accuracy: 0.8719 - val_loss: 0.3717 - val_accuracy: 0.8694\n",
            "Epoch 194/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3520 - accuracy: 0.8737 - val_loss: 0.3730 - val_accuracy: 0.8687\n",
            "Epoch 195/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3524 - accuracy: 0.8733 - val_loss: 0.3623 - val_accuracy: 0.8730\n",
            "Epoch 196/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3534 - accuracy: 0.8721 - val_loss: 0.3723 - val_accuracy: 0.8704\n",
            "Epoch 197/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3503 - accuracy: 0.8731 - val_loss: 0.3738 - val_accuracy: 0.8689\n",
            "Epoch 198/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3554 - accuracy: 0.8717 - val_loss: 0.3830 - val_accuracy: 0.8665\n",
            "Epoch 199/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3512 - accuracy: 0.8728 - val_loss: 0.3788 - val_accuracy: 0.8686\n",
            "Epoch 200/200\n",
            "235/235 [==============================] - 2s 7ms/step - loss: 0.3549 - accuracy: 0.8719 - val_loss: 0.3795 - val_accuracy: 0.8676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8fd009d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cm3fjDEMNHO"
      },
      "source": [
        "test = pd.read_csv('test.csv').reset_index(drop=True)\n",
        "test_ids = test['id']\n",
        "test = test.drop(columns=['id', 'fiberID'])\n",
        "test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMgaShW6MW82"
      },
      "source": [
        "pred_mat = model.predict(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFAX6WhfMXDz"
      },
      "source": [
        "sample = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "submission = pd.DataFrame(pred_mat, index=test.index)\n",
        "submission = submission.rename(columns=i2lb)\n",
        "submission = pd.concat([test_ids, submission], axis=1)\n",
        "submission = submission[sample.columns]\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HgAmR-wMXKA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHLBYKT-MXdJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}